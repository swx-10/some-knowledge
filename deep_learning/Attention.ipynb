{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力机制  \n",
    "## Scaled Dot-Product Attention  \n",
    "一个注意力函数，它的输入是Query、Key、Value，都是向量；经过注意力计算后，输出Output也是向量。  \n",
    "![点积注意力](picture/scaled_dot_product_attention.jpg)  \n",
    "Scales Dot-Product Attention(上图)，它包含以下组成部分：  \n",
    "1、第一步，Query乘以Key，公式为$Q*K^T$。  \n",
    "2、第二步，缩放，$Q*K^T$之后除以$\\sqrt{d_k}$,其中d是Key的维度，即$\\frac{Q*K^T}{\\sqrt{d_k}}$。  \n",
    "3、第三步（可选），乘以Attention的掩码Mask。  \n",
    "4、第四步。取Softmax，把上面计算出来的值，变成0-1之间，且它们的和为1，$Softmax(\\frac{Q*K^T}{\\sqrt{d_k}})$。  \n",
    "5、第五步，乘以Value，即$Softmax(\\frac{Q*K^T}{\\sqrt{d_k}})*V$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#缩放点积注意力\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "    #第一步\n",
    "    qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    #第二步\n",
    "    d_k = tf.cast(tf.shape(k)[-1], dtype=tf.float32)\n",
    "    scaled_attention_logits = qk / tf.sqrt(d_k)\n",
    "\n",
    "    #第三步\n",
    "    if mask is not None:\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        scaled_attention_logits += (mask*-1e9)\n",
    "\n",
    "    #第四步\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    #第五步\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention  \n",
    "相比只与使用$d_model$维度的K、V和Q执行单个注意力函数不同，将Q、K，V分别用不同的维度，线性映射h次效果更好。  \n",
    "![](picture/Multi_Head_Attention.jpg)  \n",
    "多头注意力计算步骤：  \n",
    "1、第一步，Q，K，V进行线性转化。  \n",
    "2、第二步，Q，K，V进行多头转换。  \n",
    "3、第三步，进行Attention计算。  \n",
    "4、第四步，进行多头拼接。  \n",
    "5、第五步，进行线性转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentiom(tf.keras.layers.Layer):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "\n",
    "        #线性转换\n",
    "        self.Wq = tf.keras.layers.Dense(self.d_model)\n",
    "        self.Wk = tf.keras.layers.Dense(self.d_model)\n",
    "        self.Wv = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        #第一步\n",
    "        q = self.Wq(q)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "\n",
    "        #第二步\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        #第三步\n",
    "        scaled_attention_out, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention_out = tf.transpose(scaled_attention_out, perm=[0, 2, 1, 3])\n",
    "\n",
    "        #第四步\n",
    "        concat_attention_out = tf.reshape(scaled_attention_out, (batch_size, -1, self.d_model))\n",
    "\n",
    "        #第五步\n",
    "        output = self.dense(concat_attention_out)\n",
    "        return output, attention_weights"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12077a5199a51551279b8a897e7e6cfc0637b8a793646e5ff425593f6e664ab4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('tf24')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
